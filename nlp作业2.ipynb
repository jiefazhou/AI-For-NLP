{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Review the main points of this lesson.\n",
    "** 1. How to Github and Why do we use Jupyter and Pycharm？**\n",
    "\n",
    "**Ans:** \n",
    " github是一个项目的托管平台，上面有很多的开源项目，我们可以将自己的项目上传上去，方便管理也方便别人查看。我们也可以查看他人的项目，把自己喜欢的项目fork下来，也可以给别人的项目提出意见。  通过gitbash一系列命令我们可以将本地项目上传到github的仓库里。\n",
    "\n",
    "  jupyter是一个轻量级交互式笔记本，适用于小型项目的开发，我们可以在测试项目时单独测试特定代码块，也可以利用其编写文档，非常灵活方便，所以在学习的过程中，我们验证自己的想法，运行一小段程序使用jupyter会很有优势\n",
    "\n",
    "  pycharm是一个IDE，适用于比较大型项目的开发，拥有jupyter不具备的强大调试功能，两者优势互补，所以我们需要两者的结合使用\n",
    "\n",
    "**2. What's the Probability Model?**\n",
    "\n",
    "**Ans:**\n",
    "概率模型指的是某一种分布，只有当知晓事件处于某种分布下我们才能计算此事件发生的概率。具体到课上的内容，整个词表中的词的词频分布就是一种概率模型，基于此我们能计算词语出现的概率。\n",
    "\n",
    "**3. Can you came up with some sceneraies at which we could use Probability Model?**\n",
    "\n",
    "**Ans:**\n",
    "很多分类问题都需要计算出最有可能的概率模型，最后才能进行很好的分类。在通信中，模拟数据包的到达也用到概率模型，一般的包到达率服从泊松分布，能够很好的模拟实际情况\n",
    "\n",
    "**4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?**\n",
    "\n",
    "**Ans:**\n",
    " 因为基于模式匹配等靠的是人工设计所有的可能性，只能涵盖小部分的情况且设计很困难，而且其可迁移性很差，一个方法往往只能对应一个特定的问题； 而基于概率的方法比较简单，且能够覆盖大部分可能性，更具鲁棒性，除此之外，在大数据的推动下优势也越来越明显。\n",
    "\n",
    "**5. What's the Language Model？**\n",
    "\n",
    "**Ans:**\n",
    "语言模型就是自然语言处理中用来计算一个句子的概率模型，一般来说一个句子的概率模型公式如下：\n",
    "\n",
    "\n",
    "$$pr(w_{1},w_{2},..,w_{n}) = pr(w_{1})pr(w_{2}|w_{1})..pr(w_{n}|w_{1},w_{2},..,w_{n-1})$$\n",
    "\n",
    "由于上式比较复杂所以有了N-gram语言模型，即当前词的出现概率仅仅与前面n-1个词相关，式子简化为：\n",
    "$$pr(w_{1},w_{2},..,w_{k}) = \\prod \\limits_{i=1}^{k} pr(w_{i}|w_{i-n+1},..,w_{i-1})$$\n",
    "\n",
    "**6. Can you came up with some sceneraies at which we could use Language Model?**\n",
    "\n",
    "**Ans:**\n",
    "自动生成句子，通过语言模型使生成句子的概率最大化，生成合理的句子；模糊词判断，当语音识别没听清一个词的时候，可以根据语言模型判断出可能性最大的那个词。\n",
    "\n",
    "**7. What's the 1-gram language model？**\n",
    "\n",
    "**Ans:**\n",
    "即当前词的概率和其他所有词都无关，即第五题N-gram模型公式，n = 1的情况。\n",
    "\n",
    "**8. What's the disadvantages and advantages of 1-gram language model？**\n",
    "\n",
    "**Ans:**\n",
    "上下文没有关联，只会判断当前词在词表中的频率，并不会判断词组存在的可能性，句子的顺序搭配问题无法解决。\n",
    "\n",
    "\n",
    "**9. What't the 2-gram models？**\n",
    "\n",
    "**Ans:**\n",
    "即当前词的概率和前一个词有关，即第五题N-gram模型公式，n = 2的情况。\n",
    "\n",
    "**10. what's the web crawler, and can you implement a simple crawler?**\n",
    "\n",
    "**Ans:**\n",
    "是一种按照一定的规则，自动地抓取万维网信息的程序或者脚本，它们被广泛用于互联网搜索引擎或其他类似网站，可以自动采集所有其能够访问到的页面内容，以获取或更新这些网站的内容和检索方式。\n",
    "\n",
    "**11. There may be some issues to make our crwaler programming difficult, what are these, and how do we solve them?**\n",
    "\n",
    "**Ans:**\n",
    "部分网站对 ip 进行了限制，导致我们无法爬到想要的数据，这个时候可以用代理来做。\n",
    "\n",
    "**12. What't the Regular Expression and how to use?**\n",
    "\n",
    "**Ans:**\n",
    "正则表达式是用来检查，匹配，和替换字符串符合某种条件的子串。\n",
    "正则表达式有很多规则，当我们需要取出一个字符串中的一些我们需要的子串时，我们就可以找出他们的共同点，用相对应的正则表达式去匹配他们"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Using Wikipedia dataset to finish the language model.\n",
    "\n",
    "取60M做测试\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "database = \"D:/研究僧/nlp/data of nlp/wikidata/AA/wiki_00\"\n",
    "database1 = \"D:/研究僧/nlp/data of nlp/wikidata/AA/wiki_01\"\n",
    "f = open(database,encoding=\"UTF-8\")\n",
    "f1 = open(database1,encoding=\"UTF-8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = f.read()\n",
    "data = data + f1.read()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "繁简转化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from hanziconv import HanziConv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提取所有汉字和数字"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datalist = data.split(\"}\")\n",
    "import re\n",
    "def token(string):\n",
    "    return ' '.join(re.findall('[\\u4e00-\\u9fa5|\\d]+', string))\n",
    "all_articles = [token(str(a))for a in datalist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of text: 20765737\n"
     ]
    }
   ],
   "source": [
    "text = ''\n",
    "\n",
    "for a in all_articles:\n",
    "    text += a\n",
    "print('length of text: {}'.format(len(text)))\n",
    "text = HanziConv.toSimplified(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'13 13 数学 数学 数学是利用符号语言研究数量 结构 变化以及空间等概念的一门学科 从某种角度看属于形式科学的一种 数学透过抽象化和逻辑推理的使用 由计数 计算 量度和对物体形状及运动的观察而产生'"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "def cut(string): return list(jieba.cut(string))\n",
    "TEXT = text\n",
    "ALL_TOKENS = cut(TEXT)\n",
    "valida_tokens = [t for t in ALL_TOKENS if t.strip() and t != 'n']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['13',\n",
       " '13',\n",
       " '数学',\n",
       " '数学',\n",
       " '数学',\n",
       " '是',\n",
       " '利用',\n",
       " '符号语言',\n",
       " '研究',\n",
       " '数量',\n",
       " '结构',\n",
       " '变化',\n",
       " '以及',\n",
       " '空间',\n",
       " '等',\n",
       " '概念',\n",
       " '的',\n",
       " '一门',\n",
       " '学科',\n",
       " '从']"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "valida_tokens[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('的', 598611),\n",
       " ('在', 154713),\n",
       " ('年', 124154),\n",
       " ('是', 109790),\n",
       " ('和', 98486),\n",
       " ('了', 77756),\n",
       " ('为', 68626),\n",
       " ('与', 55423),\n",
       " ('有', 52549),\n",
       " ('月', 51632),\n",
       " ('中', 45930),\n",
       " ('也', 43180),\n",
       " ('他', 39816),\n",
       " ('被', 39256),\n",
       " ('而', 34533),\n",
       " ('等', 32443),\n",
       " ('日', 31523),\n",
       " ('并', 30641),\n",
       " ('上', 30394),\n",
       " ('以', 29401)]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valida_tokens)\n",
    "from collections import Counter\n",
    "words_count = Counter(valida_tokens)\n",
    "words_count.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10016670"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "frequences_all = [f for w, f in words_count.most_common()]\n",
    "frequences_sum = sum(frequences_all)\n",
    "frequences_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prob(word): \n",
    "    esp = 1 / frequences_sum\n",
    "    if word in words_count: \n",
    "        return words_count[word] / frequences_sum\n",
    "    else:\n",
    "        return esp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from functools import reduce\n",
    "def product(numbers):\n",
    "    return reduce(lambda n1, n2: n1 * n2, numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def language_model_one_gram(string):\n",
    "    words = cut(string)\n",
    "    return product([get_prob(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.800922516663795e-18"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model_one_gram('广交会下个月举办')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2-gram\n",
    "# $pr(w_{1}|w_{2}) = \\frac{pr(w_{1},w_{2})}{pr(w_{1})}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_2_grams_words = [''.join(valida_tokens[i:i+2]) for i in range(len(valida_tokens[:-2]))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "_2_gram_sum = len(all_2_grams_words)\n",
    "_2_gram_counter = Counter(all_2_grams_words)\n",
    "\n",
    "def get_combination_prob(w1, w2):\n",
    "    if w1 + w2 in _2_gram_counter: return _2_gram_counter[w1+w2] / _2_gram_sum\n",
    "    else:\n",
    "        return 1 / _2_gram_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_prob_2_gram(w1, w2):\n",
    "    return get_combination_prob(w1, w2) / get_prob(w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def langauge_model_of_2_gram(sentence):\n",
    "    sentence_probability = 1\n",
    "    \n",
    "    words = cut(sentence)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0: \n",
    "            prob = get_prob(word)\n",
    "        else:\n",
    "            previous = words[i-1]\n",
    "            prob = get_prob_2_gram(previous, word)\n",
    "        sentence_probability *= prob\n",
    "    \n",
    "    return sentence_probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.4255325140948988e-16"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langauge_model_of_2_gram('小明今天抽奖抽到一台苹果手机')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "明天晚上请你吃大餐，我们一起吃苹果 is more possible\n",
      "---- 明天晚上请你吃大餐，我们一起吃日料 with probility 3.395719226432817e-26\n",
      "---- 明天晚上请你吃大餐，我们一起吃苹果 with probility 3.395719226432817e-26\n",
      "真是一只好看的小猫 is more possible\n",
      "---- 真事一只好看的小猫 with probility 4.49852312480122e-19\n",
      "---- 真是一只好看的小猫 with probility 5.731117316680636e-16\n",
      "今晚我去吃火锅 is more possible\n",
      "---- 今晚我去吃火锅 with probility 8.872045588782534e-14\n",
      "---- 今晚火锅去吃我 with probility 4.7715785863200155e-15\n",
      "养乐多绿来一杯 is more possible\n",
      "---- 洋葱奶昔来一杯 with probility 9.543412410564297e-12\n",
      "---- 养乐多绿来一杯 with probility 9.983361729341481e-08\n",
      "特朗普会见金正恩 is more possible\n",
      "---- 特朗普会见火星访客 with probility 6.09551948485872e-13\n",
      "---- 特朗普会见金正恩 with probility 4.1083793124862053e-10\n",
      "三十年后核战争爆发 is more possible\n",
      "---- 三十年后气候变暖 with probility 1.863958872795241e-13\n",
      "---- 三十年后核战争爆发 with probility 1.0651193558829948e-12\n"
     ]
    }
   ],
   "source": [
    "\n",
    "need_compared = [\n",
    "    \"明天晚上请你吃大餐，我们一起吃日料 明天晚上请你吃大餐，我们一起吃苹果\",\n",
    "    \"真事一只好看的小猫 真是一只好看的小猫\",\n",
    "    \"今晚我去吃火锅 今晚火锅去吃我\",\n",
    "    \"洋葱奶昔来一杯 养乐多绿来一杯\",\n",
    "    \"特朗普会见火星访客 特朗普会见金正恩\",\n",
    "    \"三十年后气候变暖  三十年后核战争爆发\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 =  langauge_model_of_2_gram(s1), langauge_model_of_2_gram(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3-Gram\n",
    "$$pr(S) = pr(w_{1})*\\frac{pr(w_{1},w_{2})}{pr(w_{1})}*\\frac{pr(w_{1},w_{2},w_{3})}{pr(w_{2},w_{3})}*..\\frac{pr(w_{n-2},w_{n-1},w_{n})}{pr(w_{n-1},w_{n})}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_3_grams_words = [''.join(valida_tokens[i:i+3]) for i in range(len(valida_tokens[:-3]))]\n",
    "_3_gram_sum = len(all_3_grams_words)\n",
    "_3_gram_counter = Counter(all_3_grams_words)\n",
    "def get_3_comb_prob(w1, w2, w3):\n",
    "    \"Return Pr(w1 * w2 * w3) if w1+w2+w3 occured in the text corpus\"\n",
    "    if w1 + w2 + w3 in _3_gram_counter:\n",
    "        return _3_gram_counter[w1+w2+w3] / _3_gram_sum\n",
    "    else:\n",
    "        return 1 / _3_gram_sum  \n",
    "    \n",
    "def get_prob_3_gram(w1, w2, w3):\n",
    "    \"Return Pr(w1 * w2 * w3) / Pr(w2 * w3) if w2+w3 and w1+w2+w3 occured in the text corpus\"\n",
    "    return get_3_comb_prob(w1, w2, w3) / get_combination_prob(w2, w3)\n",
    "\n",
    "def langauge_model_of_3_gram(sentence):\n",
    "    \"Return Pr(S) = Pr(w1) * [Pr(w1 * w2) / Pr(w1)] * [Pr(w1 * w2 * w3) / Pr(w2 * w3)] ...\"\n",
    "    sentence_prob = 1\n",
    "    words = cut(sentence)\n",
    "    \n",
    "    for i, word in enumerate(words):\n",
    "        if i == 0:\n",
    "            prob = get_prob(word)\n",
    "        elif i == 1:\n",
    "            previous = words[i-1]\n",
    "            prob = get_prob_2_gram(previous, word)\n",
    "        else:\n",
    "            word_n_2 = words[i-2]\n",
    "            word_n_1 = words[i-1]\n",
    "            prob = get_prob_3_gram(word_n_2, word_n_1, word)\n",
    "        sentence_prob *= prob\n",
    "        \n",
    "    return sentence_prob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.983363722691911e-08"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "langauge_model_of_3_gram(\"小明今天抽奖抽到一台苹果\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "明天晚上请你吃大餐，我们一起吃苹果 is more possible\n",
      "---- 明天晚上请你吃大餐，我们一起吃日料 with probility 1.1884962745320208e-10\n",
      "---- 明天晚上请你吃大餐，我们一起吃苹果 with probility 1.1884962745320208e-10\n",
      "今晚火锅去吃我 is more possible\n",
      "---- 今晚我去吃火锅 with probility 7.130972663815556e-09\n",
      "---- 今晚火锅去吃我 with probility 2.495840681504199e-08\n",
      "特朗普会见火星访客 is more possible\n",
      "---- 特朗普会见火星访客 with probility 9.983361729341779e-08\n",
      "---- 特朗普会见金正恩 with probility 9.983360732666863e-08\n",
      "三十年后核战争爆发 is more possible\n",
      "---- 三十年后气候变暖 with probility 1.1409556262104889e-07\n",
      "---- 三十年后核战争爆发 with probility 7.986689383473423e-07\n"
     ]
    }
   ],
   "source": [
    "need_compared = [\n",
    "    \"明天晚上请你吃大餐，我们一起吃日料 明天晚上请你吃大餐，我们一起吃苹果\",\n",
    "    \"今晚我去吃火锅 今晚火锅去吃我\",\n",
    "    \"特朗普会见火星访客 特朗普会见金正恩\",\n",
    "    \"三十年后气候变暖  三十年后核战争爆发\"\n",
    "]\n",
    "\n",
    "for s in need_compared:\n",
    "    s1, s2 = s.split()\n",
    "    p1, p2 = langauge_model_of_3_gram(s1), langauge_model_of_3_gram(s2)\n",
    "    \n",
    "    better = s1 if p1 > p2 else s2\n",
    "    \n",
    "    print('{} is more possible'.format(better))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s1, p1))\n",
    "    print('-'*4 + ' {} with probility {}'.format(s2, p2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compared to the previous learned parsing and pattern match problems. What's the advantage and disavantage of Probability Based Methods?# \n",
    "\n",
    "**Ans：**\n",
    "程序更简单，更容易设计，不用考虑很多情况，核心算法只需要一个语言模型，就能处理上周的问题，鲁棒性更好"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Optional) How to solve OOV problem?\n",
    "If some words are not in our dictionary or corpus. When we using language model, we need to overcome this out-of-vocabulary(OOV) problems. There are so many intelligent man to solve this probelm.\n",
    "\n",
    "\n",
    "## Q1: How did you solve this problem in your programming task?\n",
    "\n",
    "**Ans:**\n",
    "对于没有出现的词就设它的词频为1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Q2: Read about the 'Turing-Good Estimator', can explain the main points about this method, and may implement this method in your programming task\n",
    "\n",
    "**Ans: **\n",
    "Turning-Good 估计的原理就是对于没有看见的事件，我们不能认为它的发生概率就是零，因此我们从概率的总量中，分配一个很小的比例给予这些没有看见的事件，这样一来，看见的那些事件的概率总和就要小于1，因此，需要将所有看见的事件概率小一点。\n",
    "\n",
    "以统计词典的每个词的概率为例,假设在语料库中出现r次的词有Nr个，特别的未出现的词数为N0，预料库的大小为N。出现r次的词在整个语料库中的概率则是Pr = r/N，假定当r比较小时，它的统计可能不可靠，因此出现r次的那些词在计算它们概率时要使用一个更小一点的次数，当r小于等于T时，进行如下调整使出现r次词汇概率变小，即： \n",
    "\n",
    "$$ P_{r} = \\frac{(r+1)N_{r+1}}{NN_{r}}$$\n",
    "\n",
    "此时未出现词概率为下调的概率：\n",
    "$$ P_{0} = \\sum \\limits_{r=1} ^{T}\\frac{rN_{r}}{N} - \\sum \\limits_{r=1} ^{T}\\frac{(r+1)N_{r+1}}{N} = 1/N(N_{1}-(T+1)N_{T+1}) $$\n",
    "\n",
    "其中：$$N = \\sum \\limits_{r=1} ^{\\infty}rN_{r} $$\n",
    "表示所有词词频之和\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency_count = Counter(frequences_all)\n",
    "def get_prob_GoodTurning(word): \n",
    "    N_1 = frequency_count[1]\n",
    "    N_11 = frequency_count[11]\n",
    "    if word in words_count: \n",
    "        if words_count[word]>10:\n",
    "            return words_count[word]/frequences_sum\n",
    "        else:\n",
    "            N_r = frequency_count[ words_count[word]]\n",
    "            N_r1 = frequency_count[ words_count[word]+1]\n",
    "            return (words_count[word]+1)* N_r1/ (frequences_sum*N_r)\n",
    "    else:\n",
    "        return (N_1-11*N_11)/frequences_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def language_model_one_gram_GoodTurning(string):\n",
    "    words = cut(string)\n",
    "    return product([get_prob_GoodTurning(w) for w in words])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.8126894273571935e-14"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "language_model_one_gram_GoodTurning('我是一个好人')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
